from configs.settings import BaseSettings
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from configs.settings import BaseSettings, OllamaSettings
from langchain.prompts import PromptTemplate

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embedding_model = HuggingFaceEmbeddings(model_name=BaseSettings.vector_model)

# è¼‰å…¥ FAISS å‘é‡è³‡æ–™åº«ï¼ˆå•Ÿç”¨ pickle ååºåˆ—åŒ–ï¼‰
vector_db = FAISS.load_local(
    BaseSettings.vector_index_path,
    embedding_model,
    allow_dangerous_deserialization=True
)

# è‡ªå®šç¾© Prompt æ¨¡æ¿
def get_custom_prompt():
    """
    å‰µå»ºè‡ªå®šç¾© Prompt æ¨¡æ¿ï¼Œæä¾›æ›´å¥½çš„æŒ‡å°çµ¦ AI
    
    è¿”å›:
        PromptTemplate: è‡ªå®šç¾© Prompt æ¨¡æ¿
    """
    template = """
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼ŒåŸºæ–¼æä¾›çš„è³‡æ–™åº«çŸ¥è­˜å›ç­”å•é¡Œã€‚

    è«‹éµå¾ªä»¥ä¸‹åŸå‰‡ï¼š
    1. ä»”ç´°åˆ†ææ‰€æä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Š
    2. æä¾›æº–ç¢ºã€ç›¸é—œä¸”æœ‰å¹«åŠ©çš„å›ç­”
    3. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰è¶³å¤ è³‡è¨Šå›ç­”å•é¡Œï¼Œè«‹èª å¯¦èªªæ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç·¨é€ ç­”æ¡ˆ
    4. å›ç­”æ‡‰è©²ç°¡æ½”æ˜ç­ï¼Œä½†è¦åŒ…å«è¶³å¤ çš„ç´°ç¯€
    5. åªèƒ½ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”

    èŠå¤©æ­·å²ï¼š
    {chat_history}

    ä¸Šä¸‹æ–‡è³‡è¨Šï¼š
    {context}

    å•é¡Œï¼š{question}

    å›ç­”ï¼š
    """
    return PromptTemplate.from_template(template)

# å»ºç«‹ RAG Chain
def create_rag_chain():
    """
    å»ºç«‹ RAG Chain

    è¿”å›:
        ConversationalRetrievalChain: RAG Chain ç‰©ä»¶
    """
    # ä½¿ç”¨ LangChain Retriever
    retriever = vector_db.as_retriever(search_type="similarity", search_kwargs={"k": 5})

    # åŠ å…¥ Chat è¨˜æ†¶
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # å»ºç«‹ RAG Chain
    llm = Ollama(model=OllamaSettings.model_llama_3_1, base_url=OllamaSettings.service_url)
    
    # ä½¿ç”¨è‡ªå®šç¾© Prompt
    custom_prompt = get_custom_prompt()

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": custom_prompt}
    )
    return qa_chain

# é–‹å§‹èŠå¤©
def chat_with_ai(qa_chain, question):
    """
    ä½¿ç”¨ RAG Chain é€²è¡Œå•ç­”

    åƒæ•¸:
        qa_chain: RAG Chain ç‰©ä»¶
        question: æŸ¥è©¢æ–‡æœ¬

    è¿”å›:
        str: AI å›ç­”
    """
    response = qa_chain.invoke({"question": question})
    return response["answer"]

# åŸ·è¡Œ
def run_process():
    qa_chain = create_rag_chain()
    while True:
        query = input("\nğŸ’¬ ä½ æƒ³å•ä»€éº¼ï¼Ÿï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")
        if query.lower() == "exit":
            break
        answer = chat_with_ai(qa_chain, query)
        print("\nğŸ¤– AI å›ç­”ï¼š", answer)

if __name__ == "__main__":
    run_process()